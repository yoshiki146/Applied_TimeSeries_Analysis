---
title: "Ordinary Least Square estimations"
output:
  github_document:
    toc: true
    toc_depth: 2
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Introduction  
In this tutorial, we deal with the regression models in time series by ordinary least square. In time series OLS, it is often important to appropriately adjust the standard error since it is common that data has autocorrelation or heteroscedasitity. Wald F test is used instead of standard F test to examine a linear combination of variables. This is a method to incorporate the heteroscedasticity or autocorrelation to a standard F test.  

```{r import-libraries, message=F, warning=F}
library(magrittr)
library(tidyverse)
library(xts)
```


### Simple Ordinary Least Square  
At first, we consider simple ordinary least square, where we regress first order difference of spread onto first order difference of call rate. Let $\Delta y_t$ represent `spread` and $\Delta x_t$ a vector of 1s and `call`, then the model to estimate is 

\begin{equation}
  \Delta y_t =\Delta x'_t\delta + \epsilon_t
\end{equation}

where $\delta= (\delta_1,\delta_2)'$ is a vector of coefficient and $\epsilon_t$ is assumed to be i.i.d.

#### OLS estimation  
In R, `lm` command fits linear models to estimate the coefficients. Note `lm` constructs standard error under the assumption that residuals are i.i.d.  
```{r simple-ols}
diffsXts<- readRDS("~/Documents/GitHub/Applied_TimeSeries_Analysis/dat/diffsXts.rds")
ols<- lm(spread_diff~call_diff, data=diffsXts)
summary(ols)$coef
```
Intercept turns out to be insignificant but `call_diff` is signigicant in 0.1% level indicationg it is important in predicting spread. Notice, however, that we constructed standard error assuming homogeneity, which is a speacial case in economic/financial data. To see if this assumption is satisfied, we plot the residuals of the OLS model.  

```{r resid-plot, message=F}
ggplot() + 
  geom_point(aes(index(diffsXts), resid(ols))) +
  labs(x="date", y="residuals")
```
  
It seems that there is a period of high diviation (e.g. late 80s) and low diviation (e.g. 83-85). This is a typical characteristic in financial and economic series called volatility clustering.  

#### Heteroscedasticity consistent standard error  
Now, we adjust the standard error to be robust to heterogeneous distribution of residuals by modifying variance covariance matrix. `sandwich` and `lmtest` package offers flexible options for handling things like heteroscedasticity and autocorrelaiton.  

```{r heteroscedasticity-consistent-SE, message=F}
lmtest::coeftest(ols, vcov.=sandwich::vcovHC)
```
  
The standard error is larger than homogenous-only SE, hence smaller p-value. Nonetheless, `call_diff` is significant in 0.1% level.  
  
#### Heteroscedasticity and autocorrelation consistent satandard error  
Another commonly used standard error is HAC (heteroscedasticity and autocorrelation consistent) estimator. This is an extension of HC estimator above to adjust not only to heteroscedasticity but also autocorrelation. Here, we use HAC proposed by Newey and West (1987), which R provides as `sandwich::NeweyWest`.  

```{r HAC}
lmtest::coeftest(ols, vcov.=sandwich::NeweyWest)
sandwich::bwNeweyWest(ols)
```
  
The default kernel is `Bertlett`. The bandwidth to be selected is 1.59 and the result is more or less similar to the one by HC estimation.  

#### Autocorrelation test for model residuals  
To determine whether to consider autocorrelation, we examine the autocorrelation in the residuals. First, we draw autocorrelation correlation function using R's `acf` command.  

```{r acf}
acf(ols$resid, lag.max = 12)
```
  
Only lag four (and parhaps lag one) exceeds the threshold, but we cannot entirely sure if serial correlation exists. Next, we see whether a group of autocorrelations is significantly different from zero, using Ljung-Box test. We consider groups of 1, 2, 6 and 12 lags.  

```{r Ljung-Box test}
for (i in c(1,2,6,12)){
  print(Box.test(ols$resid, lag=i, type='L'))
}
```
  
In all groups considered, the tests fail to reject the null hypothesis of no sereal dependencies at 5% signicicance level.  

### Multiple OLS
We consider a multiple OLS by adding call rate of $t-1$ and $t-2$ periods. Hence $\Delta x_t$  in Equation 1 is now a vector of four length; 1s, call rate at period $t$, $t-1$ and $t-2$.  

First, as we did in the simple OLS, we estimate the coefficients under homoscedasticity assumption.  
```{r}
ols_m<-lm(spread_diff~call_diff+lag(call_diff)+lag(call_diff,2), data=diffsXts)
summary(ols_m)$coef
```

  
The result shows that the lagged variables are not significant at 5% level and coefficient for `call_diff` doesn't change much when lags are included. Note that you can change the standard error to HC or HAC as we did in the simple OLS (results omitted)  

### Wald F test  
F test provides a joint test of multiple linear restrictions about the coefficitet. Wald F test is an extension of F test that incorporates heteroscedasticity and autocorrelation. (see p.205 of Hamilton (1994) for more)  
As examples, we test two null hypothesis; (i) $\hat \delta_2 = \hat \delta_3 = \hat \delta_4=0 $ and (ii) $\hat \delta_3 + \hat \delta_4=0$.  

I first create a function (`WaldFstat`) which returns the Wald F statistics. This follows chi-squared distribution with degrees of dreedom being the number of restrictions. In put arguments are R (m\*k matrix or vector of length k), beta (a set of coef), r (m\*1 vector or scaler of values that we believe these liear combinations take on), vcov (variance covariance matrix) and q (number of restrictions). 
```{r WaldFstat}
waldFstat=function(R, beta, r, vcov,q){
  Fstat=(t(R%*%beta-r) %*% solve(R%*%vcov%*%t(R)) %*% (R%*%beta-r))/q 
  print(Fstat[1,1])
}
```
  
(i)\ $\hat \delta_2 = \hat \delta_3 = \hat \delta_4=0 $
```{r WaldF-one}
R=matrix(c(0,1,0,0, 0,0,1,0, 0,0,0,1),ncol=4, byrow=T)
beta=matrix(coef(ols_m))
r=matrix(c(0,0,0))
vcov=sandwich::vcovHAC(ols_m)
q=3
waldFstat(R, beta, r, vcov,q)
qf(0.95,q,Inf) # critical value
```
  
(ii)\ $\hat \delta_3 + \hat \delta_4=0$
```{r WaldF-two}
R=matrix(c(0,0,1,1), nrow=1,byrow=T)
beta= matrix(coef(ols_m))
r=0
vcov=sandwich::vcovHAC(ols_m)
q=1
waldFstat(R,beta, r, vcov,q)
qf(0.95,q,Inf)
```
  
The results reject the null hypothesis in (i) but doesn't in (ii), the expected result from the multiple linear regression.  

  







